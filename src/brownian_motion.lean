--import probability_space
import measure_theory.lebesgue_measure
import topology.compact_open

/- 

### $\sigma$-algebra a/k/a measurable space ###

Let $\Omega$ be a set.  A $\sigma$-*algebra* in $\Omega$ is a nonempty collection ${\cal G}$ of subsets of $\Omega$ such that 

* $\Omega \in {\cal G}$
* If $A \in {\cal G}$ then $\Omega-A \in {\cal G}$
* If $(\forall n \in {\mathbb N})\, A_n \in {\cal G}$, then $\cup_{i=1}^\infty A_i \in {\cal G}$

A pair $(S,\sigma(S))$ of a non-empty set and its $\sigma$-algebra is called a *measurable space*. 
a member $T \in \sigma(S)$ is called a *measurable set*.

https://en.wikipedia.org/wiki/%CE%A3-algebra
https://en.wikipedia.org/wiki/Measurable_space

-/

#check @measurable_space -- measurable_space : Type u_1 ‚Üí Type u_1


/- œÉ set Œ± 
### $\sigma$-algebra generated by a family of subsets of $\Omega$ ###

Let $H$ be any family of subsets of $\Omega$.  
The unique smallest $\sigma$-algebra which contains every set in $H$, written $\sigma(H)$, 
is called the $\sigma$-*algebra generated by* $H$.

https://math.stackexchange.com/questions/2456416/what-is-a-countably-generated-sigma-algebra-cant-find-a-definition-online
https://en.wikipedia.org/wiki/%CE%A3-algebra#%CF%83-algebra_generated_by_an_arbitrary_family

-/

#check @measurable_space.generate_from -- : Œ† {Œ± : Type u_1}, set (set Œ±) ‚Üí measurable_space Œ±

/-
### Measurable function ###

A function $f:S\rightarrow T$, where $(S,\sigma(S))$ and $(T,\sigma(T))$ are 
measurable spaces is said to be $(\sigma(S),\sigma(T))$-*measurable* 
if for all $B \in \sigma(T)$ then $f^{-1}(B) \in \sigma(S)$. 
The set of real-valued measurable functions is called ${\cal L}^0$. [Zitkovic][4] 

https://en.wikipedia.org/wiki/Measurable_function

-/

#check measurable -- measurable : (?M_1 ‚Üí ?M_2) ‚Üí Prop

/-
### Measure ###

$\mu:{\cal F}\rightarrow {\mathbb R}$ is a *signed real-valued measure* in the measurable space $(\Omega,{\cal F})$ iff for any sequence of disjoint sets $\{A_n\}_{n=1}^\infty \in {\cal F}$, $\mu(\cup_{n=1}^\infty A_n) = \sum_{n=1}^\infty\mu(A_n)$.

A function $\mu$ is a *measure* iff $\mu(A)\geq 0$ for all $A \in{\cal F}$, and then we say $(\Omega,{\cal F},\mu)$ is a *measure space*.  The measure $\mu(A)$ defines an integral of a nonnegative measurable function $f(\omega)$ by

$$\int_{\omega\in\Omega} f(\omega) d\mu(\omega) = \lim_{h\rightarrow 0} \lim_{N \rightarrow \infty} \sum_{n=0}^N n h \mu(\{\omega \in \Omega: nh \leq f(\omega) \leq (n+1) h\})$$
-/

#check @measure -- measure : Œ† {Œ± : Sort u_1}, (Œ± ‚Üí ‚Ñï) ‚Üí Œ± ‚Üí Œ± ‚Üí Prop

/-
### Probability measure ###

A positive measure $P:{\cal F}\rightarrow[0,1]$ such that $P(\Omega)=1$ is called a *probability measure*.
-/

#check measure_theory.probability_measure -- : measure_theory.measure ?M_1 ‚Üí Prop

/-
### Brownian trajectories and events ###

The space $\Omega$ of *elementary events* for Brownian motion is the set of all 
continuous real functions $\Omega = \{\omega: {\mathbb R}_+ \rightarrow {\mathbb R}\}$.
We refer to the $\omega(t)$ as *Brownian trajectories*.  
-/

@[derive [topological_space, has_coe_to_fun]]
def Brownian_event_space := C(nnreal, ‚Ñù)
local notation `Œ©` := Brownian_event_space

noncomputable instance : measurable_space Œ© := borel _
instance : borel_space Œ© := ‚ü®rfl‚ü©

/- 
A *cylinder set* of Brownian trajectories for a finite set of times $0\leq t_1<t_2<\cdots<t_n$ 
and real intervals $I_k=(a_k,b_k),1\leq k \leq n$ is constructed as 
$$C(t_1,\ldots,t_n;I_1,\ldots,I_n)=\{\omega\in \Omega: \forall\,1\leq k \leq n: \omega(t_k) \in I_k\}$$
 -/

def interval (n : ‚Ñï ) (I : (fin n) ‚Üí ‚Ñù √ó ‚Ñù) (k: (fin n)) := set.Ioo (I k).1 (I k).2

def cylinder_set (n : ‚Ñï) (t : fin n ‚Üí nnreal) 
                 (I : (fin n) ‚Üí ‚Ñù √ó ‚Ñù ) :=
  {œâ : Brownian_event_space | ‚àÄ k : fin n, (œâ (t k) ‚àà (interval n I k))}

#check cylinder_set -- : Œ† (n : ‚Ñï), (fin n ‚Üí nnreal) ‚Üí (fin n ‚Üí ‚Ñù √ó ‚Ñù) ‚Üí set Œ©

/-
The space of *Brownian events* ${\cal F} \subseteq 2^\Omega$ 
consists all sets of Brownian trajectories obtained from cylinders 
by operations of countable unions, intersections and complement.  
${\cal F}$ is a $\sigma$-algebra of $\Omega$:

$${\cal F}=\sigma(\{C(t_1,\ldots,t_n;I_1,\ldots,I_n): 
n \in {\mathbb Z}^+: 0\leq t_1<\cdots<t_n, I_k=(a_k,b_k),1\leq k \leq n\})$$

By definition, the elements of ${\cal F}$ are measurable sets. 
-/
œÉ 
/-
### Lebesgue integral, ${\cal L}^1$ ###

Let $f\in{\cal L}_+^{Simp,0}$.  The *Lebesque integral* 
$$\int f d\mu = \sum_{i=1}^n \alpha_k \mu(A_k) \in [0,\infty)$$
where 
$$f=\sum_{k=1}^n \alpha_k {\mathbb 1}_{A_k}$$ is a simple-function representation of $f$.  

Let $f\in{\cal L}_+^0([0,\infty])$.  The *Lebesque integral* 

$$\int f d\mu = \sup \{  \int gd\mu: g \in {\cal L}_+^{Simp,0}, g(x) \leq f(x), \forall x \in S\} \in [0,\infty]$$

Let $f^+=\max{(f,0)},f^-=\max{(-f,0)}$.  A function $f\in{\cal L}^0$ is said to be *integrable* if $\int f^+ d\mu < \infty$ and $\int f^- d\mu < \inf$.  The collection of all integrable functions in ${\cal L}^0$ is written ${\cal L}^1$. [Zitkovic][5] 
-/


/-
### Weiner probability measure ###

Let the *Weiner* probability measure $P$, where $I=(a,b)$ and $0=t_0<t_1<t_2<\cdots<t_n$ and $I_k,1\leq k\leq n$ are real intervals, be:

$$P(C(t;I))=\frac{1}{\sqrt{2\pi t}}\int_a^b e^{\frac{-x^2}{2 t}} dx$$

$$P(C(t_1,t_2,\cdots,t_n;I_1,I_2,\ldots,I_n))=\int_{I_1} \int_{I_2}\cdots\int_{I_n} \prod_{k=1}^n \frac{dx_k}{\sqrt{2\pi(t_k-t_{k-1})}} \exp\big(-\frac{(x_k-x_{k-1})^2}{2(t_k-t_{k-1})}\big)$$
-/

/-
### Brownian motion probability space

The [probability space of Brownian motion][1], $W=(\Omega,{\cal F}\subseteq 2^\Omega,P:{\cal F}\rightarrow [0,1])$.
-/

/-
### Brownian motion ###

Let $B_t(\omega)$ be a time-indexed family of random variables which are Brownian motions.    $B$ has signature $B: {\mathbb R}^+ \rightarrow V$. Let $X:V$. 

Then *mathematical Brownian motion* $B_t(\omega) = \omega(t)$ is an infinite family of random variables  $B:{\mathbb R}_+ \rightarrow V$ for which, in the Brownian motion probability space $W$,

1. $B_0(\omega)=0$ with probability 1

2. $B_t(\omega)$ is almost surely a continuous function of $t$

3. $\forall t,s \geq 0$, the increment $\Delta B_s(\omega)=B_{t+s}(\omega)-B_t(\omega)$ is independent of ${\cal F}_t$ is a zero mean Gaussian random variable with variance ${\mathbb E}|\Delta B_s|^2 = s$.
-/

/-
### Random variable ###

A *random variable* $X(\omega) \in (\Omega, {\cal F}, P)$ is a real function $X:\Omega \rightarrow {\mathbb R}$ such that ${\omega \in \Omega : X(\omega) \leq x} \in {\cal F}, \forall\,x \in {\mathbb R}$.  Write $V=\Omega \rightarrow {\mathbb R}$ for the signature of random variables and write $X:V$.
-/

/-
### Topology ###

A *topology* on a set $S$ is a family $\tau$ of subsets of $S$ which contains $\emptyset$ and $S$ and is closed under finite intersections and countable or uncountable unions.  The elements of $\tau$ are called *open sets*.  The structure $(S,\tau)$ is called a *topological space*.
-/

/-
### The topology of real numbers ###

Let ${\cal O}$ be the set of all open sets $(a,b)$ where $-\infty < a < b < \infty$.  Then $({\mathbb R},{\cal O})$ is a topology of real numbers.
-/

/-
### Borel $\sigma$-algebra ###

If $(S,\tau)$ is a topological space, then $\sigma(\tau)$, generated by all open sets, is called the Borel $\sigma$-algebra on $S$.
-/

/- 
### Borel algebra on the reals ###

$${\cal B}({\mathbb R}) = \sigma({\cal O})$$
 -/

 /-
### $\sigma$-algebra generated by a random variable ###

Let $X$ be a random variable.  The $\sigma$-*algebra generated by* $X$ is

$$\sigma(X) = \{X^{-1}(A) : A \in {\cal B}({\mathbb R})$$
-/

/-
### Expectation ###

A real-valued integral in $W=(\Omega,{\cal F},P)$ of a random variable $X(\omega)$ with respect to probability measure $P$ is called an *expectation* and written

$${\mathbb E}_W X=\int_{\omega \in \Omega} X(\omega) d P(\omega)$$

The signature is ${\mathbb E}_W: V \rightarrow{\mathbb R}$.
-/

/-
### Indicator function ###

For any set $A\in\Omega$, the *indicator* of $A$ is a random variable defined as

$${\mathbb 1}_A(\omega) = \cases{ 1 & if $\omega \in A$\\  0 & otherwise }$$

The signature is ${\mathbb 1}: {\cal F} \rightarrow \Omega \rightarrow \{0,1\}$.
-/

/-
### Multiplication of two random variables ###

Let $(X \star Y)(\omega)=X(\omega) Y(\omega)$.
-/

/-
### Event which is an atom ###

Let $\mu$ be a measure. A set $A \subseteq \Omega$ is called an *atom* if $\mu(A) > 0$ and for any measurable subset $B \subset A$ with $\mu(B) < \mu(A)$, $\mu(B) = 0$.
-/

/-
### Atoms of a random variable ###

Let $Y$ be a random variable.  The *atoms of* $Y$ are

$${\cal A}(Y) =\{\{x : Y(x) = a\}: a \in Y^{-1}(\Omega)\}$$

*TBD*: Show that each element of ${\cal A}(Y)$ is an event which is an atom.
-/

/-
**Conditional expectation: Implicit construction**

Assume we are working in $W=(\Omega,{\cal F},P)$.  Let 

* ${\cal G}$ be a $\sigma$-algebra contained in ${\cal F}$

* $X$ be a random variable

* $Z$ be ${\cal G}$-measurable

Define the *conditional expectation of* $X$ *with respect to* ${\cal G}$, written $Z={\mathbb E}_W(X|{\cal G})$, to be a random variable $Z\in {\cal L}^1$ such that [Zitkovic][6] 

* $Z$ is ${\cal G}$-measurable

* For all $A \in {\cal G}$, ${\mathbb E}_W(Z \star{\mathbb 1}_A) = {\mathbb E}_W(X \star {\mathbb 1}_A)$. 

This definition of ${\mathbb E}_W(X|{\cal G})$ is *implicit*.  The signature is ${\mathbb E}_W(X|{\cal G}):V$.
-/

/-
**Conditional expectation: Explicit construction in discrete case**

Let's look at an example from [Zitkovic][6] in the finite domain for insight that may lead us to an explicit construction for ${\mathbb E}_W(X|{\cal G})$:

Let $W=(\Omega,{\cal F},{\mathbb P})$ where $\Omega=\{a,b,c,d,e\}$, $F=2^\Omega$ and ${\cal P}$ is uniform.  Let $X=\{a\mapsto 1,b\mapsto 3,c\mapsto 3,d\mapsto 5,e\mapsto 5,f\mapsto 7\}$, $Y=\{a\mapsto 2,b\mapsto 2,c\mapsto 1,d\mapsto 1,e\mapsto 7,f\mapsto 7\}$.  Let ${\cal G}=\sigma(Y)$. Then $Z={\mathbb E}_W(X|\sigma(Y))=\{a\mapsto 2,b\mapsto 2,c\mapsto 4,d\mapsto 4,e\mapsto 6,f\mapsto 6\}$ is explicitly constructed as follows:

1. Construct the set of *atoms* ${\cal A}(Y) =\{\{x : Y(x) = a\}: a \in Y^{-1}(\Omega)\}$.

2. For each $A \in {\cal A}(Y)$, for each $a \in A$, set $Z(a) = \frac{{\mathbb E}_W(X\star {\mathbb 1}_A)}{{\mathbb P}(A)}$

In the example above, the atoms of ${\cal G}=\sigma(Y)$ are ${\cal A}(Y)=\{\{a,b\}, \{c,d\},\{e,f\}\}$ and ${\mathbb P}(A)=\frac{1}{3}$ for $A \in {\cal A}(Y)$. Also ${\mathbb E}_W(X\star {\mathbb 1}_{\{a,b\}})=\frac{2}{3}$,
${\mathbb E}_W(X\star {\mathbb 1}_{\{c,d\}})=\frac{4}{3}$ and
${\mathbb E}_W(X\star {\mathbb 1}_{\{e,f\}})=\frac{6}{3}$.  So for $a \in \{a,b\}$, $Z(a)=\frac{\frac{2}{3}}{\frac{1}{3}}=2$, and similarly for $\{c,d\}$ and $\{e,f\}$, so finally $Z=\{a\mapsto 2,b\mapsto 2,c\mapsto 4,d\mapsto5,e\mapsto 6,f\mapsto 6\}$.
-/

/-
**Conditional expectation: Explicit construction in continuous case**

Let $W=(\Omega,{\cal F},{\mathbb P})$ be the Brownian probability space.  Let ${\cal G} \subseteq {\cal F}$ be a sub-$\sigma$-algebra of ${\cal F}$.  Write $A_{\cal G}(\omega) \in {\cal G}$, for all $\omega \in \Omega$, for the *atom containing* $\omega$ in ${\cal G}$.  Then

$${\mathbb E}_W(X|{\cal G})(\omega)=\frac{{\mathbb E}_W(X\star {\mathbb 1}_{A_{\cal G}(\omega)})}{{\mathbb P}(A_{\cal G}(\omega))}$$
-/

/-
### Natural filtration ###

Define the family of $\sigma$-algebras ${\cal F}_t$ for $t \geq 0$ by cylinder sets confined to times $0\leq s < t$ for some fixed $t$.  So ${\cal F}_s \subset {\cal F}_t \subset {\cal F}$ if $0 \leq s < t < \infty$.  We call ${\cal F}_t$ the *Brownian filtration* generated by the Brownian events up to time $t$.  Constructively, we can say that 

$${\cal F}_t=\sigma(\{C(t_1,\ldots,t_n;I_1,\ldots,I_n): n \in {\mathbb Z}^+: 0\leq t_1<t_2<\cdots<t_n <t, I_k=(a_k,b_k),1\leq k \leq n\})$$
-/

/-
### Original problem ###

Show, using the explicit construction of continuous conditional expectation, that for all $0\leq s \leq t$,  ${\mathbb E}_W(B_t|{\cal F}_s)(\omega)=B_s(\omega), 0\leq s\leq t$.  So, show that

$${\mathbb E}_W(B_t\star {\mathbb 1}_{A_{{\cal F}_s}(\omega)}) = B_s {\mathbb P}(A_{{\cal F}_s}(\omega))$$

Let $W=(\Omega,{\cal F},P)$ be the probability space of Brownian motion. Let $V=\Omega \rightarrow {\mathbb R}$ be the signature of random variables.  Let $B_t(\omega)$ be a time-indexed family of random variables which are Brownian motions.    $B$ has signature $B: {\mathbb R}^+ \rightarrow V$. Let $X:V$. Define by Lebesque integral the *expectation* ${\mathbb E}_{W} X=\int_{\omega \in \Omega} X(\omega) d P(\omega)$. So ${\mathbb E}_W: V\rightarrow {\mathbb R}$. 
Let $(X \star Y)(\omega)=X(\omega) Y(\omega)$.  Let ${\cal G} \subseteq {\cal F}$.  Define, by *implicit* construction, the *conditional expectation* $Z={\mathbb E}_W(X|{\cal G})$, as the random variable $Z:V$ such that for all $A \in {\cal G}$, ${\mathbb E}_W(Z \star {\mathbb 1}_A) = {\mathbb E}_W(X \star {\mathbb 1}_A)$.  Let ${\cal F}_t \subset {\cal F}$ be the natural filtration of $B_t$. Can we show, by *explicit* construction, that ${\mathbb E}_W(B_t|{\cal F}_s)(\omega)=B_s(\omega), 0\leq s\leq t$?
-/


 /-
https://math.stackexchange.com/questions/1499020/details-about-brownian-motion

Wiener processes are often motivated by taking the limit
$$ \lim_{n\to\infty} W_n(t) = \lim_{n\to\infty} \frac{1}{\sqrt{n}} \sum_{1\leq k \leq \lfloor nt \rfloor} \xi_k,$$
where $\xi_k$ are i.i.d. random variables with mean 0 and variance 1. If we impose no further restrictions on $\xi_k$, then they might each be any real number, i.e. $\xi_k \in \mathbb{R}$. Further, we can require each $\xi_k$ to be the identity map on the underlying probability space $\Omega_k$, which induces a probability distribution $P_k$ of $\Omega_k \ni \omega_k$ via $P^{\xi_k}$. Then we can identify the elements $\omega_k$ as precisely the numbers in $\mathbb{R}$, i.e. $\Omega_k \simeq \mathbb{R}$.

Now, since $W_n(t)$ depends on $n$ different $\xi_k$, via the identifications we just made, the underlying probability space of $W_n(t)$ must be $\mathbb{R}^n$. At this point you can note that the underlying probability space of the continuous Wiener process $W(t)$ cannot be $\mathbb{R}$: The cardinality of that set would be far too small.

In the following, I'll draw a heuristic comparison to the way physicists usually treat the transition from finite-dimensional Hilbert spaces to infinite-dimensional Hilbert spaces (see $^\ast$): Intuitively, in taking the limit $n\to\infty$, we must somehow get to "$\mathbb{R}^\infty$". By the fact that the incremental distribution of the Wiener process shall be
$$W(t+\delta t) - W(t) \sim \mathcal{N}(0, \delta t),$$
we see that for $\delta t \to 0$, the probability of "large" jumps becomes vanishingly small. Therefore, the probability of discontinuities occurring also becomes vanishingly small, and the correct way of obtaining "$\mathbb{R}^\infty$" in this case should be the set of (a.e.$^{\ast\ast}$) continuous functions. Since $t$ is typically allowed to take values from 0 to infinity, we can identify the probability space of the Wiener process as $\Omega = C[0,\infty)$. In the posts you linked to, this is called the canonical realization of Brownian motion, since, along the way, we implicitly defined $W(t,\cdot)$, considered as a map from the probability space to another measurable space, to be the identity map on $\Omega = C[0,\infty)$.

___________

$^{\ast}$In introductions to quantum physics, infinite-dimensional Hilbert spaces are often introduced in the same way. Then the appropriate infinite-dimensional "limit" turns out to be the space of square-integrable functions $L^2(\mathbb{R})$, or its extension as a rigged Hilbert space.
___________

$^{\ast\ast}$Concerning continuity vs. a.e. continuity (see also [this quant. finance post](https://quant.stackexchange.com/questions/16693/why-is-brownian-motion-merely-almost-surely-continuous)), I think that's largely a matter of definition and construction. The construction I gave above does allow for discontinuities, but the probability of such a discontinuous path occurring is vanishing. Suppose $\omega$ is such a path with a discontinuity at $t_0$. Then
$$W(t_0+\delta t) - W(t_0) \overset{!}{\sim} \mathcal{N}(0,\delta t),$$
and therefore the corresponding density function is
$$p(\delta x) \propto \exp\left(-\frac{\left(\delta x\right)^2}{2\delta t}\right),$$
but since the discontinuity means that $\left|\delta x\right| > 0$ even as $\delta t \to 0$, it follows that $p(\delta x) \to 0$ and thus the probability of this particular path $\omega$ occurring also vanishes, and we are free to ignore any discontinuous paths.

 -/

 /-
 
 I am reading a number of papers by different authors which are introductions to stochastic differential equations.  All of these papers define the Wiener process $W_t$ (Brownian motion) quite simply by a few properties such as

* $W_0=0$ with probability 1
* $E(W_t)=0$
* $Var(W_t-W_s) = t-s$

where $W_t$ is [a family of real random variables indexed by the set of nonnegative real numbers $t$][1].

Most of them go on to employ $W_t$ in a stochastic differential equation in which some probability space $(\Omega,{\mathbb A},P)$ is implicitly assumed in the sense that the SDE is defined pathwise for $\omega \in \Omega$ in a form such as 

$$dX_t(\omega)=f(t,X_t(\omega)) dt + g(t,X_t(\omega)) dW_t(\omega)$$

Discussion then proceeds without explicitly constructing the probability space containing paths $\omega$.  Searching for "probability space of Wiener process" gives nothing.  Searching for "probability space of Brownian motion" gives many resources, such as [this one][2], which, as an exercise, states that "the collection of random variables $(\prod_t)_{t\in{\mathbb R}_+}$ defined on the probability space $(C[0,\infty),{\mathbb B}(C[0,\infty)),\mu)$ is a Brownian motion".  It seems like quite a lot of machinery in complex analysis and measure theory is required to construct the space.

*Question:* What is the simplest and most intuitive, easiest-to-explain construction of the probability space of paths $\omega$ invoked but not defined in the typical presentation of the basic form of an SDE?   I am trying to explain this to myself and others.  I'm hoping for an explanation which, while not pretending that measure theory and complex analysis don't exist, makes minimal use of measure theoretic constructions to define the space.  I'm not looking for proofs, just a construction of the space which would be invoked in a proof.

*Possible directions:* 

 - The [MIT notes][3] say "the sample space $\Omega$ is barely mentioned because we can identify $\omega \in \Omega$ with $B_\omega$ a continuous function".  Beyond the notational confusion, this would make $\Omega$ a function space.  
 - In [this question][4], $\Omega$ is understood to be any uncountable set, such as the real numbers, as an index into that function space, to be constructed, in one of several ways (Fourier series, random walks, and maybe more).
 - This [Math StackExchange question][5] also expresses confusion about $\Omega$ and the accepted answer "gets rid of" $\Omega$.
 -/

 /-
 Oliver Diaz

 In applications of Probability theory, the probabilistic space is seldom  specified, it sits there in the background; however, at least conceptually, one may still what key characteristics the underlying space are based on the kinds of things we are observing,  and the kinds of things we want to measure.

For theoretical purposes, one often needs to have a precise description of the underlying probability space in order to use known results, verify conditions, or further advance the theory (new theorems, concepts, etc). 

It turns out that most theoretical results can be obtained by considering the Steinhaus space $$((0,1),\mathscr{B}(0,1),\lambda)$$
where $\mathscr{B}(0,1)$ is the Borel $\sigma$-algebra in $(0,1)$, and $\lambda$ is the Lebesgue measure (length measure) restricted to the interval $(0,1)$, as the underlying  probability space ( a *canonical probabilty space* of sorts). By that I mean that one can *explicitly* generate **random samples** with values  any prescribed distribution, as well as represent **conditional expectation** by **randomization** (generation of uniform distributions).

The problem  of existence an generation of **stochastic processes** is a more subtle problem; however, one may use copies of $((0,1),\mathscr{B}(0,1))$ with a consistent prescription of finite dimensional distributions to explicitly define a stochastic process  on the product of copies of $((0,1),\mathscr{B}(0,1)$ with the prescribed finite dimensional distributions.

Here is an attempt to give a an overview of all this.

---
1. Generation of i.i.d. Bernoulli random variables (tossing a fair coin):

First notice that  in the Steinhause space, the function $\theta(x)=x$ is obviously **uniformly distributed $U[0,1]$**, that is $\lambda[\theta\leq x] =x$, for all $0<x<1$.

Recall that every $x\in[0,1]$ has a unique binary expansion 
$$x=\sum_{n\geq1}r_n/2^n$$
 where $r_n\in\{0,1\}$, and
$\sum_{n\geq1}r_n=\infty$ for $x>0$. 
For each $n\in\mathbb{N}$, the $n$--th bit map $x\mapsto
r_n(x)$ defines a measurable function from
$([0,1],\mathscr{B}([0,1]))$ to
$(\{0,1\},2^{\{0,1\}}))$, where $2^{\{0,1\}}$ is the collection of all subsets of $\{0,1\}$.

 Therefore, the map
$\beta:[0,1]\rightarrow\{0,1\}^{\mathbb{N}}$ given by
$x\mapsto(r_n(x))$ is measurable. 

The next result is a mathematical
formulation of tossing a fair coin.

**Lemma 1:** Suppose $\theta\sim U[0,1]$,
 and let $\{X_n=r_n\circ\theta\}$ 
its binary expansion.  Then,  $\{X_n\}$ is an **i.i.d.  Bernoulli
 sequence** with rate $p=\tfrac12$.
Conversely, if $(X_n)$ is an i.i.d.  Bernoulli sequence with rate 
$p=\tfrac12$,
then $\theta=\sum_{n\geq1}2^{-n}X_n\sim U[0,1]$.

Here is a short proof:

Suppose that $\theta\sim U(0,1)$. For any $N\in\mathbb{N}$ and 
$k_1,\ldots,k_N\in\{0,1\}$, 
$$\begin{align}
\bigcap^N_{j=1}\{x\in(0,1]:r_j(x)=k_j\}&=&(\sum^N_{j=1}\tfrac{k_j}{2^j},
\sum^N_{j=1}\tfrac{k_j}{2^j}+\tfrac{1}{2^N}]\\
\{x\in(0,1]:
   r_N(x)=0\}&=&\bigcup^{2^{N-1}-1}_{j=0}(\tfrac{2j}{2^N},\tfrac{2j+1}{2^N}]\\
\{x\in(0,1]:r_N(x)=1\}&=&\bigcup^{2^{N-1}-1}_{j=0}
(\tfrac{2j+1}{2^N},\tfrac{2(j+1)}{2^N}]
\end{align}
$$
It follows immediately that 
$
\mathbb{P}[\bigcap^N_{j=1}\{X_j=k_j\}]=\tfrac{1}{2^N}=\prod^N_{j=1}\mathbb{P}[X_j=k_j]$.
Hence  $\{X_n\}$ is a Bernoulli sequence with rate $\tfrac12$.


Conversely, suppose $\{X_n:n\geq1\}$ is a Bernoulli sequence with rate
$\tfrac12$. If $\widetilde{\theta}\sim U(0,1)$, then the first part
shows that the sequence of bits
$\{\widetilde{X}_n\}\stackrel{law}{=}\{X_n\}$. Therefore,
$$
\theta:=\sum_{n\geq1}2^{-n}X_n\stackrel{law}{=}
\sum_{n\geq1}2^{-n}\widetilde{X}_n=\widetilde{\theta}
$$
since $\theta$ is a measurable function of $\{X_n\}$.

All this shows that on the Steinhaus space one can generate explicitly Bernoulli sequences.

---
2. Generation of i.i.d sequences of uniform distributions:

One we can generate i.i.d sequences of Bernoulli random variables defined on the Steinhaus space, we can now generate i.i.d sequences of uniform random variables also defined on the Steinhaus space.

**Lemma 2:** There exist a sequence $(f_n)$ of  measurable functions on $[0,1]$ such that
 for any $\theta\sim U[0,1]$,  $(f_n(\theta))$ is an  i.i.d  sequence
random variables with $f_1(\theta)\sim U[0,1]$.

Here is a short proof:

 Reorder the sequence $(r_m)$ of binary bit maps into a
 two--dimensional  array $(h_{n,j}:n,j\in\mathbb{N})$, and define the
 function
$f_n:=\sum_{j\geq1}\tfrac{h_{nj}}{2^j}$ on $[0,1]$ for each $n$. 
From the fist Lemma, $\{X_n=r_n\circ\theta\}$ forms a Bernoulli
 sequence with rate $p=\tfrac12$. Thus, the collections
 $\sigma(X_{nj}:j\geq1)$ are independent. By the first Lemma,  it follows that  $(f_n)$ is
 an i.i.d. sequence of $U[0,1]$ random variables.

---

3. Generation of any distribution on the real line: 

For any probability space $(\Omega,\mathscr{F},\mathbb{P})$ and random variable $X:(\Omega,\mathscr{B})\rightarrow(\mathbb{R},\mathscr{B}(\mathbb{R})$, the **law or distribution of $X$** is the measure $\mu_X$ on $(\mathbb{R},\mathscr{B}(\mathbb{R}))$ defined by 
$$\mu_X(B)=\mathbb{P}[X\in B],\quad B\in\mathscr{F}$$

One can generate a random variable $Q:((0,1),\mathbb{R}((0,1),\lambda)\rightarrow(\mathbb{R},\mathscr{B}(\mathbb{R})$ such that the law of $Q$ is $\mu_X$. This may be done by the "**quantile function**"

$$Q(t)=\inf\big\{x\in\mathbb{R}: \mathbb{P}[X\leq x]\geq t\big\},\quad 0<t<1$$
$Q$ is non-decreasing, right continuous and has left limits. More importantly, $Q$ satisfies

$$
F(x):=\mathbb{P}[X\leq x]\geq t \quad\text{iff}\quad Q(t) \leq x
$$

Form this, it follows that 
$$\lambda[Q\leq x]:=\lambda\big(\{t\in(0,1): Q(t)\leq x\}\big)=\lambda\big(\{t\in(0,1): t\leq F(x)\}\big)=F(x)$$
and so $Q$ has the same distribution function as $X$. 

Particular examples are:

- $\Phi(x)=\frac{1}{2\pi}\int^x_{-\infty}e^{-t^2/2}\,dt$. $\Phi$ is continuous and strictly monotone increasing. It has then a continuous and strictly increasing inverse. Then $Q(t)=\Phi^{-1}(t)$, $0<t<1$, is a random variable defined in the Steinhaus space that has the **Normal distributions**. 

- $F(x)=1-e^{-x}$ is strictly monotone increasing and has inverse $F^{-1}(t)=-\log(1-t)$. Then $Q(t)=F^{-1}(t)$ is a random variable defined on the Steinhaus space and has **exponential distribution**.

---

4. Generation independent sequences of  random variables with any prescribed distribution.

Using (2) and (3) we can generate in random variables with any distribution (over $(\mathbb{R},\mathscr{B}(\mathbb{R})$).

**Corollary 3.** Suppose that $(S_n,\mathscr{S}_n,\,u_n):=(\mathbb{R},\mathscr{B}(\mathbb{R}),\mu_n)$, $n\in\mathbb{N}$ are Borel probability spaces. Then, there is a map
$F:((0,1),\mathscr{B}((0,1)),\lambda)\rightarrow
(\prod_nS_n,\bigotimes_n\mathscr{S}_n)$ such that 
 the projections $p_n:\mathbf{s}\mapsto s_n$, form an
independent sequence of random variables on
$\big(\prod_nS_n,\bigotimes_n\mathscr{S}_n,\mu\big)$,
$\mu=\lambda\circ F^{-1}$,  with
$p_n\stackrel{d}{=}\mu_n$.

Here is a short proof:

Lemma 2 provides a  $U[0,1]$--distributed 
i.i.d. sequence  $(f_n)$ of random variables defined  on
the Steinhaus space.  Part 3 shows that  for each $n$,  there is a map
 $Q_n:(0,1)\rightarrow \mathbb{R}$ such that $\lambda\circ Q^{-1}_n=\mu_n$.
The map $F$ given by  $x\mapsto(Q_n(f_n(x)))$ has the stated
properties.

----
(1) through (4) illustrate that all the basic tools od Probability theory -sampling, law of large numbers for i.i.d sequences, central limit theorem for i.i.d sequences among others- can be developed  using the Steinhaus as ***canonical space***.

The next part of the presentation is more subtle and I will skip details by adding references. On one end we illustrate how conditional expectation can be performed by **randomization**; on the other end, we show how stochastic processes can be constructed.

---

5. There is a deep [result][1] in Measure theory that states that Borel sets of complete separable metric spaces are measurable isomorphic to $((0,1),\mathscr{B}(0,1))$ (if uncountable) or a to a countable subset of $((0,1),\mathscr{B})$. This provides another justification for the use of $((0,1),\mathscr{B}(0,1))$ as a canonical measurable space. Spaces that are measurably isomorphic to a Borel subset of $(0,1)$ are called Borel spaces.

In particular, in part (4) we can substitute $(\mathbb{R},\mathscr{B}(\mathbb{R}),\mu_n)$ by Borel probability spaces, for examples  $(S_n,\mathscr{B}(S_n),\mu_n)$, where $S_n$ is a complete metric space (Polish space) space equipped with its Borel $\sigma$-algebra, and $\mu_n$ a probability measure on $(S_n\mathscr{B}(S_n))$.

---

6. Regular conditional expectation:

Another deep result in Probability is the fact that if $(\Omega,\mathscr{F},\mathbb{P})$ is a probability space, and $(S,\mathscr{B}(S))$ is a Polish measurable space ( $S$ is a Polish spaced equipped with the Borel $\sigma$-algebra), and $\mathscr{A}$ is a sub $\sigma$-algebra of $\mathscr{F}$, then
there is a stochastic kernel $\nu:\Omega\times\mathscr{B}(S)\rightarrow[0,1]$ from $(\Omega,\mathscr{A})$ tp $(S,\mathscr{B}(S))$ such 
$$\nu(\omega,A)=\mathbb{P}[X\in A|\mathscr{A}]\qquad \mathbb{P}-\text{a.s.}$$
for all $A\in\mathscr{A}$. Here, the map $\omega\rightarrow\nu(\omega,A)$ is $\mathscr{A}$--measurable for any foxed $A$.

This allows for a desintegration formula

Suppose  $(S,\mathscr{S})$ is a Polish measurable space and $(T,\mathscr{T})$ beisany measurable space. Let $\mathscr{A}\subset\mathscr{F}$  sub--$\sigma$--algebra. Let $X:(\Omega,\mathscr{F})\rightarrow(S,\mathscr{S})$ be a random variables in $S$ (the observation above guarantees that $\mathbb{P}[X\in\cdot|\mathscr{A}]$ has a regular  version $\nu$). If $Y:(\Omega,\mathscr{A})\rightarrow(T,\mathscr{T})$ and $f:(S\times T,\mathscr{S}\otimes\mathscr{T})\rightarrow\mathbb{C}$ are functions such that $\mathbb{E}[|f(X,Y)|]<\infty$ then,
$$\begin{align}
\mathbb{E}[f(X,Y)|\mathscr{A}](\cdot) &=\int_S f(x,Y(\cdot))\nu(\cdot,dx)\qquad \text{$\mathbb{P}$--a.s.}\label{conditional}\\
\mathbb{E}[f(X,Y)]&=\int_\Omega\Big(\int_S f(x,Y(\omega))\nu(\omega,dx)\Big)\mathbb{P}(d\omega)\tag{7}\label{disintegration}
\end{align}
$$
If $\mathscr{A}=\sigma(Y)$ and $\mathbb{P}[X\in dx|\sigma(Y)]=\nu(Y(\omega),dx)$ for some stochastic kernel from $(T,\mathscr{T})$ to $(S,\mathscr{S})$ then, 
$$\begin{align}
\mathbb{E}[f(X,Y)|\sigma(Y)](\cdot) &= \int_S f(x,Y(\cdot))\mu(Y(\cdot),dx) \qquad\text{$\mathbb{P}$--a.s.}\\
\mathbb{E}[f(X,Y)] &=\int_\Omega\Big(\int_S f(x,Y(\omega))\mu(Y(\omega),dx)\Big)\mathbb{P}(d\omega)
\end{align}
$$
If $X$ and $Y$ are independent then, $\mu(X\in dx|\sigma(Y)](\cdot)=\mathbb{P}[X\in dx]$  $\mathbb{P}$--a.s.

---

7. Randomization:

Stochastic kernels $\nu$ from any  measure space $(T,\mathscr{T})$ to a Borel space  $(S,\mathscr{S})$ can also be generated on the Steinhaus space.

**Lemma 4**. Let $\mu$ be a stochastic kernel
  from a measure space $S$ to a Borel space $T$. 
There is a  function $f:S\otimes[0,1]\rightarrow T$ such that if $\theta\sim U[0,1]$, then the law of $f(s,\theta)$ is $\nu(s,\cdot)$.

Here is a short proof:

By part (5) it suffices to assume $(S,\mathscr{S})$ is the $((0,1),\mathscr{B}(0,1))$, for  there is bijection $\phi:(0,1),\mathscr{B}((0,1))\longrightarrow(S,\mathscr{S})$ such that $\phi$ and $\phi^{-1}$ are measurable in which case we replace $\nu$  by $\eta(s,B):=\nu(s,\phi(B))$.
Let $g:T\times (0,1):\rightarrow \mathbb{R}$ be defined as the quantile tranformation
$$g(t,s)=\inf\{x\in(0,1): \nu(t,(-\infty,x])\geq s\}$$
Since $g(t,s)\leq x$ iff $\nu(t,(-\infty,x])\geq s$,  the
 measurability of the map
$s\mapsto\nu(s,(-\infty,x])$ implies that $g$ is
 $\mathscr{T}\otimes\mathscr{B}\big((0,1)\big)$ measurable.  If $\theta\sim U[0,1]$ (for example, the identity function $\theta(t)=t$ on the Steinhaus space), then
$$
\Pr[g(\theta,t)\leq x]=\Pr[\theta\leq\nu(t,(-\infty,x])]=\nu(t,(-\infty,x])
$$
This shows that  $g(\theta,t)\sim \nu(t,dx)$. Therefore, for $f:=\phi\circ g$, $f(\theta,t)\sim\nu(t,ds)$.

---

8. Existence of stochastic process:

Suppose 
$\{(S_t,\mathscr{S}_t):t\in\mathcal{T}\}$ is a collection
of  Borel spaces. For each $\mathcal{I}\subset\mathcal{T}$. Denote by 
$(S_\mathcal{I},\mathscr{S}_I)=\big(\prod_{t\in\mathcal{I}}S_t$,
$\bigotimes_{t\in\mathcal{I}}\mathscr{S}_t\big)$ and let 
$p_{\mathcal{I}}:S_\mathcal{T}\longrightarrow S_{\mathcal{I}}$ be  the
projection $(s_t:t\in\mathcal{T})\mapsto(s_t:t\in\mathcal{I})$.
A family of probability measures
$\{\mu_\mathcal{J}:\mathcal{J}\subset\mathcal{T},\,\text{$\mathcal{J}$ finite or countable}\}$ on $\mathscr{S}_\mathcal{J}$
 is *projective* if 
$$
\mu_{\mathcal{J}}\big(\cdot\times S_{\mathcal{J}\setminus\mathcal{I}}\big)
=\mu_{\mathcal{I}}\big(\cdot\big),\qquad \mathcal{I}\subset\mathcal{J}
$$
for any finite or countable $\mathcal{J}\subset\mathcal{T}$.

A deep  theorem due to Kolmogorov establishes the existence of stochastic process 

**Theorem 5.**  Suppose
  $\{(S_t,\mathscr{S}_t):t\in\mathcal{T}\}$ is  a family of Borel spaces.
If $\{\mu_\mathcal{I}:\mathcal{I}\subset\mathcal{T},\,\text{$\mathcal{I}$
finite}\}$ is a projective family
  of probability measures on $\mathscr{S}_\mathcal{I}$,
  then there exists a unique probability
  measure $\mu$ on $\mathscr{S}_\mathcal{T}$ such that 
$$
\mu\circ p^{-1}_\mathcal{I}=\mu_\mathcal{I}
$$
for any finite $\mathcal{I}\subset\mathcal{T}$.

By Part 5, all can be made into copies of a Borel subset of $(0,1)$ or $\mathbb{R}$. In such case, the canonical space for stochastic process $\{X_t:t\in\mathcal{T}\}$ can be chosen as $\big((0,1)^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}}(0,1)\big)$ or $\big(\mathbb{R}^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}}(\mathbb{R})\big)$
-/

/-

Unfortunately I could not make it to the last probability theory lecture and now I am reading through the notes and have some troubles understanding what is going on.

So we defined Brownian motion as a stochastic process on [0,‚àû) such that

1.) ùë°‚Ü¶ùëãùë°(ùúî) is a.s. continuous 2.) ùëãùë° has stationary ùëãùë°ùëñ‚àíùëãùë°ùëñ‚àí1‚àºùëÅ(0,ùë°ùëñ‚àíùë°ùëñ‚àí1) and independent increments where ùëãùë°‚àºùëÅ(0,ùë°).
Now, we are following the book "Continuous Time Markov Processes" and I refer to the beginning of Chapter 1.7.

It says: It is most convenient in the case of Brownian motion to take the probability space Œ© to be the space ùê∂[0,‚àû) of all continuous functions ùúî(.) on [0,‚àû). This choice is natural because Brownian paths are continuous. The process is defined by ùëã(ùë°,ùúî)=ùúî(ùë°).
The ùúé- algebra ùêπ is taken to be the smallest one for which the projection ùúî‚Ü¶ùúî(ùë°) is measurable for each ùë°. Rather than having one probability measue on (Œ©,ùêπ) we now have a family (ùëÉùë•) of probability measures indexed by ùë•‚àà‚Ñù. The probability measure is the distribution of ùë•+ùêµ(.), where ùêµ is standard Brownian motion.

This is all a little bit confusing to me. So far I regarded Brownian motion as a map ùëã:Œ©‚Üí(ùê∂[0,‚àû),ùêµ(ùê∂[0,‚àû))) where ùêµ is the Borel sigma algebra on ùê∂[0,‚àû) and Œ© was a space that I did not really care about.

But now they seem to be changing Œ© which is fairly bizarre to me. Does anybody understand what they want to do and from where to where (in particular equipped with which sigma algebra and measures) they want the brownian motion to go? As it could still be that they want to denote something different with omega, I wanted to ask the experts here.

probability
probability-theory
stochastic-processes
brownian-motion
share  cite  edit  follow  flag 
edited Oct 27 '15 at 9:24
asked Oct 26 '15 at 19:59

user167575
1,21299 silver badges2222 bronze badges

The pair of comments there is related. ‚Äì Did Oct 27 '15 at 9:43

@Did but the first comment also says that Œ© is left untouched. This actually confuses me even more, as they are talking about Œ© in this book. The only thing I woud actually need is to see from where to where my Brownian motion is supposed to map according to this book. Do you think you could help me with that? ‚Äì user167575 Oct 27 '15 at 9:52 
2

As you say, one possible choice, which is the one your source is advocating, is to define the Brownian motion ùëã as the identity map on ùê∂[0,‚àû) with the suitable sigma-algebra and the suitable probability measure. That is, ùëã=(ùëãùë°) with ùëãùë°(ùúî)=ùúî(ùë°) for every ùë°. This is called the canonical realization of Brownian motion and ùê∂[0,‚àû) with the suitable sigma-algebra and the suitable probability measure is the canonical space for Brownian motion. Thus, ùëã(ùúî)=ùúî and each ùëãùë° maps ùê∂[0,‚àû) to ‚Ñù. Is this your question? ‚Äì Did Oct 27 '15 at 10:01

(All modesty aside, I was referring to my comments on the other page.) ‚Äì Did Oct 27 '15 at 10:02

yes, this was really confusing me, thank you. ‚Äì user167575 Oct 27 '15 at 10:24
add a comment
1 Answer

2

There are at least two concepts that you might want to consider separately, before you look at this definition again: the canonical process, and time continuous Markov process. They are changing Œ© because they want the process to be canonical, and they define a whole bunch of probability measures, because this is necessary to define a Markov process.

Just go step by step through the definition:

ùê∂[0,‚àû) is just a non-empty set, which is a perfectly fine carrier set for a measurable space.
For each ùë°, the projection ùëãùë°:ùê∂[0,‚àû)‚Üí‚Ñù, defined by ùëãùë°(ùëì)=ùëì(ùë°) is a well-defined function. You also know that (‚Ñù,ùîÖ(‚Ñù)) is a measurable space. Thus, you can define a ùúé-algebra Óà≤ on ùê∂[0,‚àû) as the initial ùúé-algebra induced by the family of functions {ùëãùë°}ùë°. Now you have a measurable space (ùê∂[0,‚àû),Óà≤).
Now, for each starting point ùë•‚àà‚Ñù, you define a probability measure ùëÉùë• as the law of ùë°‚Ü¶ùë•+ùêµ(ùë°), that is, the law of a Brownian motion started at point ùë•. So now, you have a measurable space (ùê∂[0,‚àû),Óà≤) and a bunch of probability measures {ùëÉùë•}ùë• on this space.
By construction, each projection ùëãùë°:ùê∂[0,‚àû)‚Üí‚Ñù is measurable, thus (ùëãùë°)ùë° is a stochastic process on (ùê∂[0,‚àû),Óà≤,ùëÉùë•) for each starting point ùë•‚àà‚Ñù. The process (ùëãùë°)ùë° is called the canonical process, because it's built from the canonical projections. By construction, ùëÉùë•[ùëã0=ùë•]=1, this is the first property in the definition of a Markov process that one has to verify.
Depending on how exactly one constructed paths of a Brownian motion, one can more or less easily show that the whole construction indeed constitutes a Markov process (ùëãùë°)ùë° with distributions (ùëÉùë•)ùë•‚àà‚Ñù on the space (ùê∂[0,‚àû),Óà≤).
The process (ùëãùë°)ùë° with distributions (ùëÉùë•)ùë•‚àà‚Ñù on the space (ùê∂[0,‚àû),Óà≤) is the Brownian motion explicitly written down as a time-continuous Markov process.

Here is a simpler construction that might help to understand why we can get rid of Œ© and make everything canonical: Suppose you have a probability space (Œ©,Óà≠,ùëÉ), a measurable space (Œ®,ÓàÆ), and an Óà≠‚àíÓàÆ-measurable random variable ùëã. This variable induces a measure on (Œ®,ÓàÆ) as follows: ùêµ‚ààÓàÆ‚Ü¶ùëÉ(ùëã‚àí1(ùêµ)). Thus, (Œ®,ÓàÆ,ùëÉ‚àòùëã‚àí1) also becomes a probability space. The identity function ùêºùëë:Œ®‚ÜíŒ® is trivially measurable, thus we can interpret the identity function ùêºùëë as a Œ®-valued random variable. Furthermore, notice that ùêºùëë and ùëã have the same law, because ùëÉ[ùëã‚ààùêµ]=ùëÉ[ùëã‚àí1(ùêµ)]=(ùëÉ‚àòùëã‚àí1)[ùêµ]=(ùëÉ‚àòùëã‚àí1)[ùêºùëë‚àí1(ùêµ)]=(ùëÉ‚àòùëã‚àí1)[ùêºùëë‚ààùêµ]. Therefore, we can actually get rid of the original Œ©, forget ùëã, forget ùëÉ, and instead build a new probability measure on (Œ®,ÓàÆ) and use the identity function on Œ® instead of ùëã.

share  cite  edit  follow  flag 
edited Oct 27 '15 at 10:27
answered Oct 27 '15 at 10:14


Andrey Tyukin
76744 silver badges1111 bronze badges

very interesting and very helpful! So the Brownian motion comes via the (ùëÉùë•)ùë• into the game. ùëÉùë•[ùëã0=ùë•]=ùëÉ(ùëã0(ùë•+ùêµ(ùë°))=ùë•)=ùëÉ(ùë•+ùêµ(0)=ùë•)=1 as Brownian motion starts at zero, right? Now talking about this sigma algebra Óà≤, is this the same sigma algebra as the canonical Borel sigma algebra induced by the supremum norm or is this sigma-algebra different? ‚Äì user167575 Oct 27 '15 at 10:23
1

Yes, it comes in through the (ùëÉùë•)ùë•. I assume that it has already been constructed previously (e.g. by Levy's construction), the quote in your question is concerned with "re-packaging" the already constructed paths of Brownian motion so that it fits into the time-continuous Markov-processes framework. I added another paragraph to show how one can "forget annoying implementation details" in a simpler case. What about the sigma-algebra: see
-/


/-

1. Kallenberg's, Foundations of modern probability covers the probabilitistic aspects of 1 to 8. His proves can be consider probabilistic (as oppose purely to measure theoretic). In particular his proof of Kolmogorov's extension relies in purely probabilistic constrictions.  
2. Parthasaraty's, Probability on Metric spaces is a good reference for the measurable isomorphic theorem that in essence reduces any nice probability space to the measurable space $((0,1),\mathscr{B}(0,1))$.
3. Leo Breiman's classic Probability covers also beautifully Kolmogorov's extension theorem and many aspects of the points I discussed above.

[1]: http://www.springer.com/cda/content/document/cda_downloaddocument/9781441916044-c1.pdf?SGWID=0-0-45-855433-p173940722

[2]: http://galton.uchicago.edu/~lalley/Courses/383/ConditionalExpectation.pdf

[3]: https://math.stackexchange.com/questions/211233/generate-the-smallest-sigma-algebra-containing-a-given-family-of-sets

[4]: https://www.ma.utexas.edu/users/gordanz/notes/measurable_spaces.pdf

[5]: https://www.ma.utexas.edu/users/gordanz/notes/lebesgue_integration.pdf

[6]: https://www.ma.utexas.edu/users/gordanz/notes/conditional_expectation.pdf

[7]: https://en.wikipedia.org/wiki/Atom_(measure_theory)

  [1]: http://galton.uchicago.edu/~lalley/Courses/385/BrownianMotion.pdf
  [2]: http://math.iisc.ernet.in/~manju/MartBM/Lectures-part4.pdf
  [3]: http://math.mit.edu/~jerison/103/handouts/brownian.13.pdf
  [4]: https://www.physicsforums.com/threads/cardinality-of-sample-space-for-brownian-motion.571824/
  [5]: https://math.stackexchange.com/questions/1499020/details-about-brownian-motion

  [1]: http://math.iisc.ac.in/~manju/MartBM/RaoSrivastava_borelisomorphism.pdf
 -/